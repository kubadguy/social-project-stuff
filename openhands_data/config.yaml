# config.yaml

llm:
  provider: openai
  api_base: http://local:11434/v1
  model: llama3.2
  api_key: ollama
  temperature: 0.5
